{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import polyfingerprints as pfp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wrapconfig import YAMLWrapConfig\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load curated dataset and info\n",
    "curated_csv =  \"./expanded_data.csv\"\n",
    "df = pd.read_csv(curated_csv, sep=\",\", decimal=\".\")\n",
    "\n",
    "infofile = YAMLWrapConfig(\"expanded_info.yml\")\n",
    "infofile.load()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "additional_columns=infofile[\"numerical_columns\"]+infofile[\"categorical_columns\"]\n",
    "y=[\"cloud_point\"]\n",
    "\n",
    "DEFAULT_PFPDATA={\n",
    "    \"intersection_fp_size\":256,\n",
    "    \"enhanced_sum_fp_size\":256,\n",
    "}\n",
    "\n",
    "hyperparameter=YAMLWrapConfig(\"hyperparameter.yml\")\n",
    "hyperparameter[\"pfp_data\"].fill(DEFAULT_PFPDATA)\n",
    "pfpdata = pfp.loader.df_loader(\n",
    "        df=df,\n",
    "        repeating_unit_columns=tuple(\n",
    "            zip(\n",
    "                infofile[\"repeating_unit_columns\"],\n",
    "                infofile[\"molpercent_repeating_unit_columns\"],\n",
    "            )\n",
    "        ),\n",
    "        y=y,\n",
    "        mw_column=\"Mn\",\n",
    "        start_group_column=\"SMILES_start_group\",\n",
    "        end_group_column=\"SMILES_end_group\",\n",
    "        additional_columns= additional_columns,\n",
    "        intersection_fp_size=hyperparameter[\"pfp_data\"][\"intersection_fp_size\"],\n",
    "        enhanced_sum_fp_size=hyperparameter[\"pfp_data\"][\"enhanced_sum_fp_size\"]\n",
    "    )\n",
    "\n",
    "pfpdata, reduced_fp_data = pfp.reduce_pfp_in_dataset(pfpdata)\n",
    "# Save the dictionary of arrays for later likewise reduction to a .npz file\n",
    "np.savez('pfp_reduction.npz', **reduced_fp_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pfpdata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#for training y is necessary, so remove entries without y (cloud point)\n",
    "data = [d for d in pfpdata if d[\"y\"] is not None]\n",
    "x, y = pfp.loader.to_input_output_data(data)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BASEMODEL_DIR=\"models\"\n",
    "MODELNAME=\"test4\"\n",
    "hyperparameter=YAMLWrapConfig(\"hyperparameter.yml\")\n",
    "\n",
    "modeldir=os.path.abspath(os.path.join(BASEMODEL_DIR,MODELNAME))\n",
    "if not os.path.exists(modeldir):\n",
    "    os.makedirs(modeldir)\n",
    "\n",
    "from polyfingerprints import models as pfp_models\n",
    "DEFAULT_MODELDATA={\n",
    "    \"model\":\"FCCModel\",\n",
    "    \"model_hp\":{\n",
    "        \"activation\":\"ELU\",\n",
    "        \"lr\":1e-3,\n",
    "    },\n",
    "    \"training\":{\n",
    "        \"seed\":42,\n",
    "        \"split\":[0.8,0.1,0.1],\n",
    "        \"batch_size\":1024,\n",
    "        \"epochs\":2300,\n",
    "        \"val_metric\":\"val_loss\",\n",
    "        \n",
    "}\n",
    "}\n",
    "\n",
    "shutil.copy(hyperparameter.path,os.path.join(modeldir,\"hyperparameter.yml\"))\n",
    "hyperparameter=YAMLWrapConfig(os.path.join(modeldir,\"hyperparameter.yml\"))\n",
    "hyperparameter[\"model\"].fill(DEFAULT_MODELDATA)\n",
    "hyperparameter[\"model\"][\"model_hp\"][\"input_dim\"] = x.shape[1]\n",
    "hyperparameter[\"model\"][\"model_hp\"][\"output_dim\"] = y.shape[1]\n",
    "modelclass=getattr(pfp_models,hyperparameter[\"model\"][\"model\"])\n",
    "model = modelclass(**hyperparameter[\"model\"][\"model_hp\"])\n",
    "\n",
    "shutil.copy('pfp_reduction.npz',os.path.join(modeldir,'pfp_reduction.npz'))\n",
    "shutil.copy(infofile.path,os.path.join(modeldir,\"expanded_info.yml\"))\n",
    "shutil.copy(\"expanded_data.csv\",os.path.join(modeldir,'expanded_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, x, y, batch_size, epochs,seed,split,val_metric):\n",
    "    from pytorch_lightning import Trainer\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "    from pytorch_lightning.loggers import CSVLogger\n",
    "    from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "    \n",
    "    split=list(split)\n",
    "    if len(split)<3:\n",
    "        split=split+[0]*(3-len(split))\n",
    "    split = np.array(split[:3],dtype=float)\n",
    "    split = split/split.sum()\n",
    "    split= (split.cumsum()*len(x)).astype(int)\n",
    "\n",
    "\n",
    "    # split into train test and validation\n",
    "    rng = np.random.RandomState(seed)\n",
    "    indices = np.arange(len(x))\n",
    "    rng.shuffle(indices)\n",
    "    train_indices = indices[: split[0]]\n",
    "    test_indices = indices[split[0] : split[1]]\n",
    "    val_indices = indices[split[1] :]\n",
    "\n",
    "    x_train = torch.tensor(x[train_indices], dtype=torch.float32)\n",
    "    y_train = torch.tensor(y[train_indices], dtype=torch.float32)\n",
    "    x_test = torch.tensor(x[test_indices], dtype=torch.float32)\n",
    "    y_test = torch.tensor(y[test_indices], dtype=torch.float32)\n",
    "    x_val = torch.tensor(x[val_indices], dtype=torch.float32)\n",
    "    y_val = torch.tensor(y[val_indices], dtype=torch.float32)\n",
    "\n",
    "    train_ds = TensorDataset(x_train, y_train)\n",
    "    test_ds = TensorDataset(x_test, y_test)\n",
    "    val_ds = TensorDataset(x_val, y_val)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size)\n",
    "    test_dl = DataLoader(test_ds, batch_size=batch_size)\n",
    "    val_dl = DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(dirpath=modeldir,filename='best_model',monitor=val_metric,enable_version_counter=False)\n",
    "    modelpath=os.path.join(checkpoint_callback.dirpath,checkpoint_callback.filename+checkpoint_callback.FILE_EXTENSION)\n",
    "    training_results=YAMLWrapConfig(os.path.join(modeldir,\"training_results.yml\"))\n",
    "    \n",
    "    csv_logger=CSVLogger(modeldir,name=\"tempdir\",version=\"temp\")\n",
    "    early_stopping_cb=EarlyStopping(monitor=val_metric, min_delta=0.00, patience=200)\n",
    "    trainer = Trainer(max_epochs=epochs, log_every_n_steps=1,callbacks=[checkpoint_callback,early_stopping_cb],logger=[csv_logger])\n",
    "    trainer.fit(model, train_dl, val_dl,ckpt_path=modelpath if os.path.exists(modelpath) else None)\n",
    "    print(\"PAT\",early_stopping_cb.patience)\n",
    "    training_results[\"training_epochs\"]=trainer.current_epoch\n",
    "\n",
    "\n",
    "    model = model.__class__.load_from_checkpoint(modelpath,**hyperparameter[\"model\"][\"model_hp\"])\n",
    "    \n",
    "    if os.path.exists(os.path.join(modeldir,\"tempdir\",\"temp\",\"metrics.csv\")):\n",
    "        new_metrics = pd.read_csv(os.path.join(modeldir,\"tempdir\",\"temp\",\"metrics.csv\"))\n",
    "\n",
    "\n",
    "        if os.path.exists(os.path.join(modeldir,\"metrics.csv\")):\n",
    "            old_metrics = pd.read_csv(os.path.join(modeldir,\"metrics.csv\"))\n",
    "            new_metrics = pd.concat([old_metrics,new_metrics])\n",
    "\n",
    "        new_metrics.set_index(['epoch', 'step'], inplace=True)\n",
    "        new_metrics = new_metrics.groupby(level=['epoch', 'step']).first()\n",
    "        new_metrics = new_metrics.reset_index()\n",
    "        new_metrics = new_metrics.dropna(axis=1, how='all')\n",
    "        new_metrics.to_csv(os.path.join(modeldir,\"metrics.csv\"),index=False)\n",
    "\n",
    "\n",
    "        best_score=float(checkpoint_callback.best_model_score.cpu().numpy())\n",
    "        training_results[\"best_score\"]=best_score\n",
    "        closest = new_metrics.loc[(new_metrics[val_metric] - best_score).abs().idxmin()]\n",
    "        training_results[\"best_epoch\"]=int(closest[\"epoch\"])\n",
    "        training_results[\"best_step\"]=int(closest[\"step\"])\n",
    "        test_loss = trainer.test(model, test_dl)\n",
    "        training_results[\"test_loss\"]=list(test_loss[0].values())[0]\n",
    "\n",
    "        r=20\n",
    "    \n",
    "        for c in new_metrics.columns:\n",
    "            if c in ['epoch', 'step']:\n",
    "                continue\n",
    "            nan=new_metrics[c].isna()\n",
    "            rows=new_metrics[~nan]\n",
    "            print(len(rows))\n",
    "            plt.plot(rows['step'],rows[c].rolling(max(1,int(len(rows)/r))).mean(),\"-o\" if len(rows)<10 else \"-\" ,label=c)\n",
    "        plt.plot(training_results[\"best_step\"], training_results[\"test_loss\"],\"o\",label=\"test_loss\")\n",
    "\n",
    "       # plt.yscale(\"log\")\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(modeldir,\"metrics.png\"),dpi=300)\n",
    "            \n",
    "    shutil.rmtree(os.path.join(modeldir,\"tempdir\"))\n",
    "\n",
    "\n",
    "\n",
    "train_model(\n",
    "    model,x,y,\n",
    "    **hyperparameter[\"model\"][\"training\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}